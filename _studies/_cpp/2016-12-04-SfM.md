### SfM Method / 변화로 부터 구조 파악하는 방법

SfM : Structure from Motion

#### Refinement of the reconstruction / 재구성한 것 다듬기

* **Bundle Adjustment (BA)** : 무더기로 조정하기(?)

3D 점들의 위치와 카메라의 위치를 동시에 조정하여, reprojection / 재투영 에러를 최소화한다.

근사치로 결정된 3D 점들이 원래의 2D 점들의 위치에 가장 가까운 곳으로 투영(돌출) 된다.

이 때문에 앞에서 각 3D 점군들에 대한 원래 2D 점들의 위치를 지니고 있는 것이다.

* **Simple Sparse Bundle Adjustment (SSBA)**

BA 중에서 간단한 것으로 SSBA가 있다. 

입력 파라미터가 적기 때문에 우리의 자료구조로도 BA를 비교적 쉽게 수행할 수 있다. 

필요한 요소는 카메라 파라미터(instrinsics ?), 3D 점군들, (3D 점군과 관련된) 2D 이미지 점들, 장면을 향하고 있는 카메라들(해당안되는 카메라도 있을 수 있음)이다.

* **Source Code**

```
void BundleAdjuster::adjustBundle(
    vector<CloudPoint> & pointcloud,                        // 3D 점군
    const Mat & cam_intrinsics,                             // 카메라 고유값? (파라미터)
    conststd::vector<std::vector<cv::KeyPoint>> & imgpts,   // 2D 이미지들
    std::map<int ,cv::Matx34d> & Pmats)                     // 카메라들
{
    int N = Pmats.size(), M = pointcloud.size(), K = -1;
    cout<<"N (cams) = "<< N <<" M (points) = "<< M <<" K (measurements) =
    "<< K <<endl;
    
    StdDistortionFunction distortion;
    
    // intrinsic parameters matrix - 카메라 고유값(?) 파라미터를 담는 행렬을 초기화한다.
    Matrix3x3d KMat;
    makeIdentityMatrix(KMat);   // 행렬 초기화
    KMat[0][0] = cam_intrinsics.at<double>(0,0);
    KMat[0][1] = cam_intrinsics.at<double>(0,1);
    KMat[0][2] = cam_intrinsics.at<double>(0,2);
    KMat[1][1] = cam_intrinsics.at<double>(1,1);
    KMat[1][2] = cam_intrinsics.at<double>(1,2);
    ...
    
    // 3D point cloud - 3D 점군 위치를 담는 벡터 x 를 만든다. 개수는 점군의 개수이다.
    vector<Vector3d >Xs(M);
    for (int j = 0; j < M; ++j)
    {
        Xs[j][0] = pointcloud[j].pt.x;
        Xs[j][1] = pointcloud[j].pt.y;
        Xs[j][2] = pointcloud[j].pt.z;
    }
    cout<<"Read the 3D points."<<endl;
    
    // convert cameras to BA datastructs - N개의 카메라(행렬로 표현)를 담고 있는 벡터를 만든다.
    vector<CameraMatrix> cams(N);
    for (inti = 0; i< N; ++i)
    {
        intcamId = i;           // 카메라 번호(id)
        Matrix3x3d R;           // 카메라 회전 행렬
        Vector3d T;             // 카메라 위치(방향?) 벡터
        Matx34d& P = Pmats[i];  // 각 카메라에 대한 변환 행렬을 저장하는 행렬
        
        R[0][0] = P(0,0); R[0][1] = P(0,1); R[0][2] = P(0,2); T[0] = P(0,3);
        R[1][0] = P(1,0); R[1][1] = P(1,1); R[1][2] = P(1,2); T[1] = P(1,3);
        R[2][0] = P(2,0); R[2][1] = P(2,1); R[2][2] = P(2,2); T[2] = P(2,3);
        
        cams[i].setIntrinsic(Knorm);        // K norm - K : 카메라 고유값(?)의 놈
        cams[i].setRotation(R);
        cams[i].setTranslation(T);
    }
    
    cout<<"Read the cameras."<<endl;
    vector<Vector2d > measurements;
    vector<int> correspondingView;
    vector<int> correspondingPoint;
    
    // 2D corresponding points
    for (unsigned int k = 0; k <pointcloud.size(); ++k) // 모든 3D 점군에 대해
    {                                                   // 관련된 2D 점들을 구함(?)
        for (unsigned int i=0; i<pointcloud[k].imgpt_for_img.size(); i++) {
            if (pointcloud[k].imgpt_for_img[i] >= 0) {
                int view = i, point = k;
                Vector3d p, np;
                
                Point cvp = imgpts[i][pointcloud[k].imgpt_for_img[i]].pt;
                p[0] = cvp.x;
                p[1] = cvp.y;
                p[2] = 1.0;                             // 2D 이미지는 z값 없음
                
                // Normalize the measurements to match the unit focal length.
                scaleVectorIP(1.0/f0, p);
                measurements.push_back(Vector2d(p[0], p[1]));
                correspondingView.push_back(view);
                correspondingPoint.push_back(point);
            }
        }
    } // end for (k)
    K = measurements.size();
    cout<<"Read "<< K <<" valid 2D measurements."<<endl;
    ...
    // perform the bundle adjustment - 어떻게 하는지는 함수를 열어봐야 알 듯함. ㅜㅜ
    {
        CommonInternalsMetricBundleOptimizeropt(V3D::FULL_BUNDLE_FOCAL_LENGTH_PP,
                                                inlierThreshold, K0, distortion, cams, Xs,
                                                measurements, correspondingView, correspondingPoint);
        opt.tau = 1e-3;
        opt.maxIterations = 50;
        opt.minimize();
        cout<<"optimizer status = "<<opt.status<<endl;
    }
    ...
    //extract 3D points - 3D 점군들의 위치를 새로 저장
    for (unsigned int j = 0; j <Xs.size(); ++j)
    {
        pointcloud[j].pt.x = Xs[j][0];
        
        pointcloud[j].pt.y = Xs[j][1];
        pointcloud[j].pt.z = Xs[j][2];
    }
    //extract adjusted cameras - 각 카메라의 위치 및 방향을 새로 저장
    for (int i = 0; i< N; ++i)
    {
        Matrix3x3d R = cams[i].getRotation();
        Vector3d T = cams[i].getTranslation();
        Matx34d P;
        P(0,0) = R[0][0]; P(0,1) = R[0][1]; P(0,2) = R[0][2]; P(0,3) = T[0];
        P(1,0) = R[1][0]; P(1,1) = R[1][1]; P(1,2) = R[1][2]; P(1,3) = T[1];
        P(2,0) = R[2][0]; P(2,1) = R[2][1]; P(2,2) = R[2][2]; P(2,3) = T[2];
        Pmats[i] = P;
    }
}
```

* **결과 그림**

삼각 측량 방법으로 구한 점들 사이의 어긋남들이 잘 정리됨을 알 수 있다.

특히, 평면 등을 더 잘 재구성하는 것을 알 수 있다.

#### Visualizing 3D point clouds with PCL / PCL로 3D 점군을 시각화하기

3D 데이터로 작업하면 에러나 원본 데이터만 보면 잘 이해하기 힘들다. 

하지만, 점군 자체를 보면 그게 타당한 것인지 잘못된 것인지 즉시 알 수 있다.

시각화에는 촉망받는 프로젝트인 Point Cloud Library (PCL)를 사용할 것이다.

그외에도 다양한 도구들을 같이 사용할 것이다.

```
pcl::PointCloud<pcl::PointXYZRGB>::Ptr cloud;
void PopulatePCLPointCloud(const vector<Point3d>& pointcloud,
                           const std::vector<cv::Vec3b>& pointcloud_RGB
                           )
//Populate point cloud
{
    cout<<"Creating point cloud...";
    cloud.reset(new pcl::PointCloud<pcl::PointXYZRGB>);
    for (unsigned int i=0; i<pointcloud.size(); i++) {
        // get the RGB color value for the point
        Vec3b rgbv(255,255,255);
        if (pointcloud_RGB.size() >= i) {
            rgbv = pointcloud_RGB[i];
        }
        // check for erroneous coordinates (NaN, Inf, etc.)
        if (pointcloud[i].x != pointcloud[i].x || isnan(pointcloud[i].x) ||
            pointcloud[i].y != pointcloud[i].y || isnan(pointcloud[i].y) ||
            pointcloud[i].z != pointcloud[i].z || isnan(pointcloud[i].z) ||
            fabsf(pointcloud[i].x) > 10.0 ||
            fabsf(pointcloud[i].y) > 10.0 ||
            fabsf(pointcloud[i].z) > 10.0) {
            continue;
        }
        Visualizing 3D point clouds with PCL
        pcl::PointXYZRGB pclp;
        // 3D coordinates
        
        pclp.x = pointcloud[i].x;
        pclp.y = pointcloud[i].y;
        pclp.z = pointcloud[i].z;
        // RGB color, needs to be represented as an integer
        uint32_t rgb = ((uint32_t)rgbv[2] << 16 | (uint32_t)rgbv[1] << 8 |
                        (uint32_t)rgbv[0]);
        pclp.rgb = *reinterpret_cast<float*>(&rgb);
        cloud->push_back(pclp);
    }
    cloud->width = (uint32_t) cloud->points.size(); // number of points
    cloud->height = 1; // a list of points, one row of data
}
```

* **statistical outlier removal (SOR)**

돌출된 값-잡음-들을 제거한다.

```
Void SORFilter() {
    pcl::PointCloud<pcl::PointXYZRGB>::Ptr cloud_filtered (new pcl::PointC
                                                           loud<pcl::PointXYZRGB>);
    std::cerr<<"Cloud before SOR filtering: "<< cloud->width * cloud-
    >height <<" data points"<<std::endl;
    // Create the filtering object
    pcl::StatisticalOutlierRemoval<pcl::PointXYZRGB>sor;
    sor.setInputCloud (cloud);				// 점군 데이터 지정
    sor.setMeanK (50);						// 평균값 지정
    sor.setStddevMulThresh (1.0);			// 분산값 경계
    sor.filter (*cloud_filtered);			// 분산값이 넘치는 것을 잘라냄(?)
    std::cerr<<"Cloud after SOR filtering: "<<cloud_filtered->width *
    cloud_filtered->height <<" data points "<<std::endl;
    copyPointCloud(*cloud_filtered,*cloud);
}
```

```
Void RunVisualization(const vector<cv::Point3d>& pointcloud,
                      const std::vector<cv::Vec3b>& pointcloud_RGB) {
    PopulatePCLPointCloud(pointcloud,pointcloud_RGB);
    SORFilter();
    copyPointCloud(*cloud,*orig_cloud);
    pcl::visualization::CloudViewer viewer("Cloud Viewer");
    // run the cloud viewer
    viewer.showCloud(orig_cloud,"orig");
    while (!viewer.wasStopped ())
    {
        // NOP
    }
}
```

* **결과 그림**

잡음이 제거되서 깨끗한 점군을 얻을 수 있다.